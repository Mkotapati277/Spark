package pack
import org.apache.hadoop.hive.ql.exec.UDF
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.{Row, SparkSession, functions => F}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._
object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    // Define the data
    val data = Seq(
      (1, "Charles", "A", 1000),
      (2, "Richard", "A", 2000),
      (3, "John", "A", 2000),
      (4, "Alisha", "B", 400),
      (5, "Robin", "B", 500),
      (6, "Kara", "C", 700),
      (7, "Natalie", "D", 900),
      (8, "Harry", "C", 600),
      (9, "Charles", "D", 500),
      (10, "Kate", "A", 1000) ).toDF("EmpId", "EmpName", "Dept", "Sal")

    data.show()
val windowspec = Window.orderBy(desc("sal"))

    val highsal = data.withColumn("dense_rank",dense_rank().over(windowspec))
    highsal.show()

    val res = highsal.filter($"dense_rank" === 2)
    res.show()



    }
}
