package pack
import org.apache.hadoop.hive.ql.exec.UDF
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._

object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    val data = Seq("sahil sahoo").toDF("input_string")
    data.show()
    // Define a UDF to alternate capitalization using foldLeft
    val  alcapUDF = udf((input : String) =>
          {
          input.split(" ").map
               {
                  word =>
                   word.foldLeft((new StringBuilder, 0))
                   {
                    case ((sb, index), char: Char) =>
                      if (index % 2 == 0) sb.append(char.toUpper) else sb.append(char.toLower)
                      (sb,index+1)
                     }._1.toString()
                }.mkString(" ")
          })

val resultdf = data.withColumn("output_str", alcapUDF($"input_string"))

    resultdf.show()

  }
}
