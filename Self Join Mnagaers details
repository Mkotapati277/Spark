package pack
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._

object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    val data = Seq(
      (101, "Mary", Some(102)),
      (102, "Ravi", None),
      (103, "Raj", Some(102)),
      (104, "Pete", Some(103)),
      (105, "Prasad", Some(103)),
      (106, "Ben", Some(103))
    )

    // Define the schema
    val columns = Seq("EMPID", "EMNAME", "MNGID")

    // Create DataFrame
    val df = spark.createDataFrame(data).toDF(columns: _*)

    // Show the DataFrame
    df.show()

    val mangdf = df.as("emp").join(df.as("mgr"),$"emp.MNGID" === $"mgr.EMPID")
                   .select($"mgr.EMPID".alias("MGNID"), $"mgr.EMNAME".alias("Ename"))
    mangdf.show()
// Remove Duplicates 
    val newmangdf = mangdf.dropDuplicates().orderBy("MGNID")
    newmangdf.show()
  }
}
