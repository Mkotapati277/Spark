package pack
import org.apache.hadoop.hive.ql.exec.UDF
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.{Row, SparkSession, functions => F}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._
object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    // Create the DataFrame
    val employeeData = Seq(
      (10, "Anil", 50000, 18),
      (11, "Vikas", 75000, 16),
      (12, "Nisha", 40000, 18),
      (13, "Nidhi", 60000, 17),
      (14, "Priya", 80000, 18),
      (15, "Mohit", 45000, 18),
      (16, "Rajesh", 90000, 16),
      (17, "Raman", 55000, 16),
      (18, "Santosh", 65000, 17)).toDF("Emp_Id", "Emp_name", "Salary", "Manager_Id")
    employeeData.show()

    val df = employeeData.as("e1").join(employeeData.as("m1") , $"e1.Manager_Id" === $"m1.Emp_Id")
      .select($"e1.Manager_Id",$"m1.Emp_name".as("Manager Name"),$"m1.salary")
    df.show()
    val res = df.groupBy("Manager_Id","Manager Name").agg(sum("salary"))
    res.show()

  }
}
