package pack
import org.apache.hadoop.hive.ql.exec.UDF
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SparkSession, functions => F}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._
object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    // Define the sequence of tuples with the provided data
    val data = Seq(
      (1, "Indigo", "India", "Bhutan"),
      (2, "Air Asia", "Aus", "India"),
      (3, "Indigo", "Bhutan", "Nepal"),
      (4, "spice jet", "SriLanka", "Bhutan"),
      (5, "Indigo", "Nepal", "SriLanka"),
      (6, "Air Asia", "India", "Japan"),
      (7, "spice jet", "Bhutan", "Nepal")).toDF("id", "airway", "src", "dest")
    data.show()

    val resdf = data.groupBy("airway")
                    .agg( collect_set("src").as("source"),collect_set("dest").as("destination"))
    resdf.show(false)





  }
}
