
- List all the employee as the First Name & last L Name as full Name, return the DepartmentId if ManagerId ID is null,
 else return the ManagerId of the employee. There should be a first hire date column also whose value will be first hire_date 
of department to which the employee belongs to.



package pack
import org.apache.hadoop.hive.ql.exec.UDF
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SparkSession, functions => F}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType}
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._
object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    // Define schema for the DataFrame
    val schema = StructType(Array(
      StructField("Id", IntegerType, nullable = false),
      StructField("FName", StringType, nullable = false),
      StructField("LName", StringType, nullable = false),
      StructField("PhoneNumber", StringType, nullable = true),
      StructField("ManagerId", IntegerType, nullable = true),
      StructField("DepartId", IntegerType, nullable = false),
      StructField("Salary", IntegerType, nullable = false),
      StructField("HireDate", DateType, nullable = false)))
    // Data for Employees table
     val data = Seq(
      (1, "James", "Smith", "1234567890", null, 1, 13000, java.sql.Date.valueOf("2002-01-01")),
      (2, "John", "Johnson", "2468101214", 1, 3, 400, java.sql.Date.valueOf("2005-03-23")),
      (3, "Michael", "Williams", "1357911131", 1, 2, 16000, java.sql.Date.valueOf("2009-05-12")),
      (4, "John", "Smith", "1212121212", 2, 1, 500, java.sql.Date.valueOf("2016-07-24")),
      (5, "James", "Williams", "1234567891", null, 1, 5000, java.sql.Date.valueOf("2012-01-01")),
      (6, "John", "Williams", "2468101212", 3, 1, 3400, java.sql.Date.valueOf("2015-03-23")),
      (7, "Smith", "Williams", "1357911133", 4, 2, 6700, java.sql.Date.valueOf("2019-05-12")),
      (8, "Michael", "Smith", "1212121214", 2, 3, 1500, java.sql.Date.valueOf("2006-07-24")),
      (9, "Michael", "Johnson", "1357911135", 1, 2, 600, java.sql.Date.valueOf("2009-05-12")),
      (10, "Johnathon", "Smith", "1212121216", 2, 1, 2500, java.sql.Date.valueOf("2020-07-24")))
    // Convert Seq to RDD of Rows
    val rowRDD = spark.sparkContext.parallelize(data.map {
      case (id, fname, lname, phoneNumber, managerId, departId, salary, hireDate) =>
        Row(id, fname, lname, phoneNumber, managerId, departId, salary, hireDate) })
    // Create DataFrame
    val df = spark.createDataFrame(rowRDD, schema)
    // Show the DataFrame
    df.show()
    // Sample data for Departments
    val departmentsData = Seq(
      (1, "HR"),
      (2, "Sales"),
      (3, "Tech")).toDF("id","depart_name")
departmentsData.show()

val firsthire = df.groupBy("DepartId").agg(min("HireDate").as("first_hire"))
    firsthire.show()

    val joindf = df.join(firsthire,Seq("DepartId"),"left")
    joindf.show()

    val fildf = joindf.select(
                              concat(col("FName"),lit(""),col("LName") ).as("full name"),
      when( col("ManagerId").isNull,col("DepartId")).otherwise(col("ManagerId")).as("ManagerOrDepartmentId"),col("first_hire")
    )
    fildf.show()

  }
}
