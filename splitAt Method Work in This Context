package pack
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

import scala.io.Source
import org.apache.spark.sql.types.{HIVE_TYPE_STRING, IntegerType}
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions
import org.apache.spark.sql.expressions
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.{coalesce, col, count, dense_rank, exp, expr, monotonically_increasing_id, row_number, sum, to_date, window}
import org.apache.spark.sql.functions._

object Secanriospratice {
  def main(Args:Array[String]):Unit= {
    println("======Hello=======")
    val conf = new SparkConf().setAppName("first").setMaster("local[*]")
      .set("spark.driver.host", "localhost")
      .set("spark.driver.allowMultipleContexts", "true")
    val sc = new SparkContext(conf)
    sc.setLogLevel("ERROR")
    val spark = SparkSession.builder.getOrCreate()
    import spark.implicits._
    // Input data
    val data = Seq("abc", "for", "abc", "like", "geek1", "nerdy", "xyz", "love", "questions", "words", "life")
    val df = data.toDF("value")
    df.show()
    // Define the sizes for each group
    val groupSizes = Seq(5, 5, 1)

    // Function to partition a list into groups based on sizes
    def partitionIntoGroups[T](items: Seq[T], sizes: Seq[Int]): Seq[Seq[T]] = {
      // Recursive function to handle the partitioning
      def recursivePartition(remainingItems: Seq[T], remainingSizes: Seq[Int], resultGroups: Seq[Seq[T]]): Seq[Seq[T]] = remainingSizes match {
        case Nil => resultGroups :+ remainingItems
        case currentSize :: restSizes =>
          val (currentGroup, leftoverItems) = remainingItems.splitAt(currentSize)
          recursivePartition(leftoverItems, restSizes, resultGroups :+ currentGroup)
      }
      recursivePartition(items, sizes, Seq.empty)
    }

    // Partition the data
    val groupedData = partitionIntoGroups(data, groupSizes)

    // Convert to DataFrame
    val groupedDF = groupedData.zipWithIndex.flatMap {
        case (group, idx) => group.map(item => (idx + 1, item)) // +1 to use 1-based index
      }.toDF("group_index", "value")
      .groupBy("group_index")
      .agg(collect_list("value").as("group"))
      .orderBy("group_index")

    // Show the result
    groupedDF.show(false)
  }
}
